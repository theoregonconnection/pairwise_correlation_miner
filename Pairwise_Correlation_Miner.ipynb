{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cad3751-1a5a-41b6-b3ba-5d33f65b376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score \n",
    "from sklearn.preprocessing import PolynomialFeatures \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb64ab3e-f990-4c94-9c9a-5ac315b15235",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Define the Dataframe\n",
    "df_to_use = df_base\n",
    "\n",
    "# Define the target variables to analyze \n",
    "target_list = ['RESULT_HOME_TEAM_VICTORY', \n",
    "'RESULT_HOME_TEAM_FAVORITE', \n",
    "'RESULT_FAVORITE_VICTORY', \n",
    "'RESULT_SPREAD_OVER', \n",
    "'RESULT_SPREAD_UNDER' \n",
    "]\n",
    "\n",
    "# Define the feature variables to analyze \n",
    "feature_list = ['DIV_GAME', \n",
    "'TEMPERATURE', \n",
    "'WINDSPEED', \n",
    "'GRASS', \n",
    "'WEATHER_RAIN', \n",
    "'WEATHER_SNOW', \n",
    "'WEATHER_CLEAR', \n",
    "'WEATHER_WIND_RANK', \n",
    "'WEATHER_TEMPERATURE_RANK', \n",
    "'TREND_DIFF_HOME_OV_VS_AWAY_OV_PLAY_RANK' \n",
    "]\n",
    "\n",
    "# Specify the file path where you want to save the CSV file\n",
    "export_file_path_string = r\"C:\\Users\\theor\\OneDrive\\Desktop\\ML_Miner\\data.csv\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69f9c251-3078-41bd-a643-a226507080a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This sub function is used by the main function to check that the data submitted to the function has proper format\n",
    "def valid_data_checker(feature_df, target_df):\n",
    "\n",
    "    #Assess whether data has non-numeric values or nulls.\n",
    "    valid_data_check = 1 \n",
    "\n",
    "    # PROCESS FEATURE_DF\n",
    "    # Check for non-numeric values\n",
    "    non_numeric_columns = feature_df.select_dtypes(exclude=['number']).columns\n",
    "    if not non_numeric_columns.empty:\n",
    "        print(f\"The feature_df DataFrame contains non-numeric values in the following columns: {non_numeric_columns.tolist()}\")\n",
    "        valid_data_check = 0\n",
    "    # Check for null values\n",
    "    null_values = feature_df.isnull()\n",
    "    if null_values.any().any():\n",
    "        print(f\"There are {null_values.sum()} null values in the feature DataFrame for column: {feature_df.columns[0]}.\")\n",
    "        valid_data_check = 0\n",
    "\n",
    "    # PROCESS TARGET_DF\n",
    "    # Check for non-numeric values\n",
    "    non_numeric_columns = target_df.select_dtypes(exclude=['number']).columns\n",
    "    if not non_numeric_columns.empty:\n",
    "        print(f\"The target_df DataFrame contains non-numeric values in the following columns: {non_numeric_columns.tolist()}\")\n",
    "        valid_data_check = 0\n",
    "    # Check for null values\n",
    "    null_values = target_df.isnull()\n",
    "    if null_values.any().any():\n",
    "        print(f\"There are {null_values.sum()} null values in the target DataFrame for column: {target_df.columns[0]}.\")\n",
    "        valid_data_check = 0\n",
    "\n",
    "    # Return the continue value\n",
    "    return valid_data_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cea42063-44de-4499-bc06-3bf7106dc079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M Test for linear regression \n",
    "# The M test takes each data point and determines 2 approximate values: the probability that the point is as close to the predicted line as it is, and the probability that the point is as close to the null hypothesis line as it is. \n",
    "# The M test then runs a t test to determine the probability that those 2 buckets of probabilities are statistically different.\n",
    "\n",
    "def m_test_linear_regression(slope, intercept, feature_df, target_df):\n",
    "\n",
    "    \n",
    "    # Get the average, median, null default, min and max range for target_df\n",
    "    target_mean = target_df.iloc[:, 0].mean()\n",
    "    target_median = target_df.iloc[:, 0].median()\n",
    "    null_value = ((target_mean + target_median) / 2)\n",
    "    target_min = target_df.iloc[:, 0].min()\n",
    "    target_max = target_df.iloc[:, 0].max()\n",
    "    \n",
    "    # Create placeholders for probabilities\n",
    "    test_probabilities = []\n",
    "    null_probabilities = []\n",
    "\n",
    "    # Get the stats for the target_df \n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    while i < len(feature_df):\n",
    "        # Select the feature and target value to analyze\n",
    "        feature_value = feature_df.iloc[i, 0]\n",
    "        target_value = target_df.iloc[i, 0]\n",
    "\n",
    "        # Predict where target would be if model was true \n",
    "        predicted_target = (slope)*(feature_value) + intercept \n",
    "        \n",
    "        # Get the maximum possible distance from the predicted target value to the min/max of the possible values\n",
    "        max_possible_distance_prediction = max(abs(target_min - predicted_target), abs(target_max - predicted_target))\n",
    "        # Get the actual distance from the target value to the predicted target value \n",
    "        actual_distance_prediction = abs(target_value - predicted_target)\n",
    "        # Get the approximate probability value for predicted vs actual \n",
    "        prediction_probability = 1 - (actual_distance_prediction/max_possible_distance_prediction)\n",
    "        \n",
    "        # Get the maximum possible distance from the null hypothesis value to the min/max of the possible values\n",
    "        max_possible_distance_null_hypothesis = max(abs(target_min - null_value), abs(target_max - null_value))\n",
    "        # Get the actual distance from the target value to the null hypothesis value \n",
    "        actual_distance_null_hypothesis = abs(target_value - null_value)\n",
    "        # Get the approximate probability value for predicted vs actual \n",
    "        null_hypothesis_probability = 1 - (actual_distance_null_hypothesis/max_possible_distance_null_hypothesis)\n",
    " \n",
    "        # Append probabilities\n",
    "        test_probabilities.append(prediction_probability)\n",
    "        null_probabilities.append(null_hypothesis_probability)        \n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    # Get Results \n",
    "    avg_test_probability = sum(test_probabilities)/len(test_probabilities) \n",
    "    avg_null_probability = sum(null_probabilities)/len(null_probabilities)\n",
    "    if avg_test_probability > avg_null_probability:\n",
    "        test_probability_higher = 1\n",
    "    else:\n",
    "        test_probability_higher = 0\n",
    "\n",
    "    # Perform t-test\n",
    "    t_statistic, p_value = ttest_ind(test_probabilities, null_probabilities)\n",
    "\n",
    "    # Put results in dataframe for return\n",
    "    results_data = {'AVG_TEST_PROBABILITY': [avg_test_probability], \n",
    "                   'AVG_NULL_PROBABILITY': [avg_null_probability], \n",
    "                   'TEST_PROBABILITY_HIGHER': [test_probability_higher], \n",
    "                   'T_STATISTIC': [t_statistic], \n",
    "                   'P_VALUE': [p_value], \n",
    "                    'DATA_POINTS': [len(feature_df)]\n",
    "                  }\n",
    "\n",
    "    results_df = pd.DataFrame(results_data)\n",
    "\n",
    "    return results_df \n",
    "\n",
    "\n",
    "#Run the function \n",
    "#m_test_linear_regression(slope, intercept, f_df, t_df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dd1a718-dcce-42d1-81f2-216f0b980f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute linear regression analysis \n",
    "\n",
    "def execute_linear_regression(feature_df, target_df):\n",
    "\n",
    "    # Concatenate the two dataframes horizontally\n",
    "    data = pd.concat([feature_df, target_df], axis=1)\n",
    "    feature_name = feature_df.columns[0]\n",
    "    target_name = target_df.columns[0]\n",
    "\n",
    "    # Create a pandas series for the target variable \n",
    "    target_series = target_df[target_name]\n",
    "    \n",
    "    #Create a dataframe to hold the results of the loop\n",
    "    # Create a DataFrame with three columns\n",
    "    results_data = {\n",
    "        'slope': [],\n",
    "        'intercept': [],\n",
    "        'mse': [],\n",
    "        'r2': [], \n",
    "        'AVG_TEST_PROBABILITY': [], \n",
    "        'AVG_NULL_PROBABILITY': [],\n",
    "        'TEST_PROBABILITY_HIGHER': [],\n",
    "        'T_STATISTIC': [],\n",
    "        'M_TEST_P_VALUE': [],\n",
    "        'DATA_POINTS': []\n",
    "    }\n",
    "\n",
    "    # Create a linear regression model\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Fit the model on the data\n",
    "    model.fit(feature_df, target_series)\n",
    "\n",
    "    # Get the stats for linear regression\n",
    "    predictions = model.predict(feature_df)\n",
    "    mse = mean_squared_error(target_series, predictions)\n",
    "    r2 = r2_score(target_series, predictions)\n",
    "    slope = model.coef_[0]\n",
    "    intercept = model.intercept_\n",
    "\n",
    "    # Get the probability stats from the m test for linear regression\n",
    "    probability_stats  =  m_test_linear_regression(slope, intercept, feature_df, target_df) \n",
    "    \n",
    "    # Load results to the results_data dictionary\n",
    "    results_data['slope'].append(slope)\n",
    "    results_data['intercept'].append(intercept)\n",
    "    results_data['mse'].append(mse)\n",
    "    results_data['r2'].append(r2)\n",
    "    results_data['AVG_TEST_PROBABILITY'].append(probability_stats.loc[0, 'AVG_TEST_PROBABILITY'])\n",
    "    results_data['AVG_NULL_PROBABILITY'].append(probability_stats.loc[0, 'AVG_NULL_PROBABILITY'])\n",
    "    results_data['TEST_PROBABILITY_HIGHER'].append(probability_stats.loc[0, 'TEST_PROBABILITY_HIGHER'])\n",
    "    results_data['T_STATISTIC'].append(probability_stats.loc[0, 'T_STATISTIC'])\n",
    "    results_data['M_TEST_P_VALUE'].append(probability_stats.loc[0, 'P_VALUE'])\n",
    "    results_data['DATA_POINTS'].append(probability_stats.loc[0, 'DATA_POINTS'])\n",
    "    \n",
    "    return results_data\n",
    "\n",
    "\n",
    "#execute_linear_regression(feature_df, target_df)\n",
    "#print(feature_df)\n",
    "#print(target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7e2b30-f1b4-4aee-8f6a-c14f584fbed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7e597f0-ff2a-4361-b8a4-b23d15dfd69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M Test for 2 degree polynomial regression \n",
    "# The M test takes each data point and determines 2 approximate values: the probability that the point is as close to the predicted line as it is, and the probability that the point is as close to the null hypothesis line as it is. \n",
    "# The M test then runs a t test to determine the probability that those 2 buckets of probabilities are statistically different.\n",
    "\n",
    "def m_test_polynomial_2_regression(coefficient1, coefficient2, intercept, feature_df, target_df):\n",
    "    \n",
    "    # Get the average, median, null default, min and max range for target_df\n",
    "    target_mean = target_df.iloc[:, 0].mean()\n",
    "    target_median = target_df.iloc[:, 0].median()\n",
    "    null_value = ((target_mean + target_median) / 2)\n",
    "    target_min = target_df.iloc[:, 0].min()\n",
    "    target_max = target_df.iloc[:, 0].max()\n",
    "    \n",
    "    # Create placeholders for probabilities\n",
    "    test_probabilities = []\n",
    "    null_probabilities = []\n",
    "\n",
    "    # Get the stats for the target_df    \n",
    "    i = 0\n",
    "    while i < len(feature_df):\n",
    "        # Select the feature and target value to analyze\n",
    "        feature_value = feature_df.iloc[i, 0]\n",
    "        target_value = target_df.iloc[i, 0]\n",
    "\n",
    "        # Predict where target would be if model was true \n",
    "        predicted_target = ((coefficient1)*(feature_value)) + ((coefficient2)*((feature_value)**2)) + intercept \n",
    "  \n",
    "        # Get the maximum possible distance from the predicted target value to the min/max of the possible values\n",
    "        max_possible_distance_prediction = max(abs(target_min - predicted_target), abs(target_max - predicted_target))\n",
    "        # Get the actual distance from the target value to the predicted target value \n",
    "        actual_distance_prediction = abs(target_value - predicted_target)\n",
    "        # Get the approximate probability value for predicted vs actual \n",
    "        prediction_probability = 1 - (actual_distance_prediction/max_possible_distance_prediction)\n",
    "        \n",
    "        # Get the maximum possible distance from the null hypothesis value to the min/max of the possible values\n",
    "        max_possible_distance_null_hypothesis = max(abs(target_min - null_value), abs(target_max - null_value))\n",
    "        # Get the actual distance from the target value to the null hypothesis value \n",
    "        actual_distance_null_hypothesis = abs(target_value - null_value)\n",
    "        # Get the approximate probability value for predicted vs actual \n",
    "        null_hypothesis_probability = 1 - (actual_distance_null_hypothesis/max_possible_distance_null_hypothesis)\n",
    " \n",
    "        # Append probabilities\n",
    "        test_probabilities.append(prediction_probability)\n",
    "        null_probabilities.append(null_hypothesis_probability)        \n",
    "        \n",
    "        \n",
    "        #print(f\"Predicted Target: {predicted_target}\")\n",
    "        #print(f\"Actual Target: {target_value}\")\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    # Get Results \n",
    "    avg_test_probability = sum(test_probabilities)/len(test_probabilities) \n",
    "    avg_null_probability = sum(null_probabilities)/len(null_probabilities)\n",
    "    if avg_test_probability > avg_null_probability:\n",
    "        test_probability_higher = 1\n",
    "    else:\n",
    "        test_probability_higher = 0\n",
    "\n",
    "    # Perform t-test\n",
    "    t_statistic, p_value = ttest_ind(test_probabilities, null_probabilities)\n",
    "\n",
    "    # Put results in dataframe for return\n",
    "    results_data = {'AVG_TEST_PROBABILITY': [avg_test_probability], \n",
    "                   'AVG_NULL_PROBABILITY': [avg_null_probability], \n",
    "                   'TEST_PROBABILITY_HIGHER': [test_probability_higher], \n",
    "                   'T_STATISTIC': [t_statistic], \n",
    "                   'P_VALUE': [p_value], \n",
    "                   'DATA_POINTS': [len(feature_df)]\n",
    "                  }\n",
    "\n",
    "    results_df = pd.DataFrame(results_data)\n",
    "\n",
    "    return results_df \n",
    "\n",
    "#Run the function \n",
    "#output = m_test_polynomial_2_regression(coefficient1, coefficient2, intercept, feature_df, target_df) \n",
    "#print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fca933fd-3120-4282-8bf1-a2712663ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute 2 degree polynomial regression analysis \n",
    "\n",
    "def execute_polynomial_regression(feature_df, target_df):\n",
    "    \n",
    "    # Concatenate the two dataframes horizontally\n",
    "    data = pd.concat([feature_df, target_df], axis=1)\n",
    "    feature_name = feature_df.columns[0]\n",
    "    target_name = target_df.columns[0]\n",
    "    \n",
    "    # Create a pandas series for the target variable \n",
    "    target_series = target_df[target_name]\n",
    "    \n",
    "    #Create a dataframe to hold the results of the loop\n",
    "    # Create a DataFrame with three columns\n",
    "    results_data = {\n",
    "    'coefficient1': [],\n",
    "    'coefficient2': [],\n",
    "    'intercept': [],\n",
    "    'mse': [],\n",
    "    'r2': [], \n",
    "    'AVG_TEST_PROBABILITY': [], \n",
    "    'AVG_NULL_PROBABILITY': [],\n",
    "    'TEST_PROBABILITY_HIGHER': [],\n",
    "    'T_STATISTIC': [],\n",
    "    'M_TEST_P_VALUE': [],\n",
    "    'DATA_POINTS': []\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Define the degree of the polynomial\n",
    "    degree = 2  # You can change this as needed\n",
    "    \n",
    "    # Create polynomial features\n",
    "    poly_features = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly_features.fit_transform(feature_df)\n",
    "\n",
    "    # Fit a linear regression model to the polynomial features\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, target_series)\n",
    "\n",
    "    # Make predictions on the entire dataset\n",
    "    y_pred = model.predict(X_poly)\n",
    "    \n",
    "    # Print the coefficients of the polynomial regression equation\n",
    "    coefficients = model.coef_\n",
    "    coefficient1 = coefficients[1]\n",
    "    coefficient2 = coefficients[2]\n",
    "    intercept = model.intercept_\n",
    "\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(target_series, y_pred)\n",
    "    r2 = r2_score(target_series, y_pred)\n",
    "\n",
    "    # Get the probability stats from the m test for linear regression\n",
    "    probability_stats  =  m_test_polynomial_2_regression(coefficient1, coefficient2, intercept, feature_df, target_df) \n",
    "    \n",
    "    # Load results to the results_data dictionary\n",
    "    results_data['coefficient1'].append(coefficient1)\n",
    "    results_data['coefficient2'].append(coefficient2)\n",
    "    results_data['intercept'].append(intercept)\n",
    "    results_data['mse'].append(mse)\n",
    "    results_data['r2'].append(r2)\n",
    "    results_data['AVG_TEST_PROBABILITY'].append(probability_stats.loc[0, 'AVG_TEST_PROBABILITY'])\n",
    "    results_data['AVG_NULL_PROBABILITY'].append(probability_stats.loc[0, 'AVG_NULL_PROBABILITY'])\n",
    "    results_data['TEST_PROBABILITY_HIGHER'].append(probability_stats.loc[0, 'TEST_PROBABILITY_HIGHER'])\n",
    "    results_data['T_STATISTIC'].append(probability_stats.loc[0, 'T_STATISTIC'])\n",
    "    results_data['M_TEST_P_VALUE'].append(probability_stats.loc[0, 'P_VALUE'])\n",
    "    results_data['DATA_POINTS'].append(probability_stats.loc[0, 'DATA_POINTS'])\n",
    "    \n",
    "    return results_data\n",
    "\n",
    "#execute_polynomial_regression(feature_df, target_df)\n",
    "#print(feature_df)\n",
    "#print(target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead943c2-13c2-47bb-9e9c-d4558fe10c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9c288ab-7101-4f16-8a07-f55228062ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autobucket and Execute T Test \n",
    "# The autobucket T test automatically separates the target data into binary low vs high buckets. The threshold is mean(mean, median). Then it runs a Welches t test to asses whether the feature values from the low bucket are different than the feature values from the high bucket. \n",
    "# Note: If your data is already in binary 0/1 format, this should keep them in the same binary buckets (unless you only have 1 represented, of course). \n",
    "\n",
    "def autobucket_t_test(feature_df, target_df):\n",
    "\n",
    "     # Get the average, median and blended average of target\n",
    "    target_mean = target_df.iloc[:, 0].mean()\n",
    "    target_median = target_df.iloc[:, 0].median()\n",
    "    blended_average = ((target_mean + target_median) / 2)\n",
    "    \n",
    "\n",
    "    # Put feature variables into high vs low. \n",
    "    features_with_low_target = []\n",
    "    features_with_high_target = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(feature_df):\n",
    "        # Select the feature and target value to analyze\n",
    "        feature_value = feature_df.iloc[i, 0]\n",
    "        target_value = target_df.iloc[i, 0]\n",
    "\n",
    "        if target_value >= blended_average:\n",
    "            features_with_high_target.append(feature_value) \n",
    "        else:\n",
    "            features_with_low_target.append(feature_value)\n",
    "        i += 1\n",
    "\n",
    "    \n",
    "    number_of_high_target_features = len(features_with_high_target)\n",
    "    number_of_low_target_features = len(features_with_low_target)\n",
    "    avg_feature_value_high_target = sum(features_with_high_target)/len(features_with_high_target)\n",
    "    avg_feature_value_low_target = sum(features_with_low_target)/len(features_with_low_target)\n",
    "\n",
    "    # Perform Welch's t-test\n",
    "    t_statistic, p_value = ttest_ind(features_with_high_target, features_with_low_target, equal_var=False)\n",
    "\n",
    "    results_data = {\n",
    "    'Threshold': [blended_average],\n",
    "    'High_Target_Feature_Count': [number_of_high_target_features],\n",
    "    'Low_Target_Feature_Count': [number_of_low_target_features],\n",
    "    'High_Target_Feature_Avg': [avg_feature_value_high_target],\n",
    "    'Low_Target_Feature_Avg': [avg_feature_value_low_target], \n",
    "    'T_Statistic': [t_statistic], \n",
    "    'p_Value': [p_value]\n",
    "    }\n",
    "\n",
    "    return results_data\n",
    "\n",
    "#autobucket_t_test(feature_df, target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2636e36b-4921-4781-91c8-775677f54993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10 feature values and 5 target values for a total of 50 combinations.\n",
      "Currently processing target 1 of 5 total targets.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theor\\AppData\\Local\\Temp\\ipykernel_12728\\4129034773.py:93: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataloader_df = pd.concat([dataloader_df, pd.DataFrame([dataloader])], ignore_index=True, sort=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete and results exported to: C:\\Users\\theor\\OneDrive\\Desktop\\ML_Miner\\data.csv\n"
     ]
    }
   ],
   "source": [
    "# This is the main function. You feed it a dataframe with all of your data, a list of the columns you want to treat as features, a list of columns to treat as targets and the location where you want to export the results to. \n",
    "\n",
    "def execute_feature_target_pair_analysis(df_to_use, feature_list, target_list, export_file_path_string): \n",
    "    \n",
    "    try:\n",
    "        # Validate that inputs are dataframe, list, list. Exit the function if they are not valid. \n",
    "        proceed = 1\n",
    "        if not isinstance(df_to_use, pd.DataFrame):\n",
    "            print(f\"FAILURE: The first object (dataframe with all data) submitted to the function must be a dataframe. You are currently submitting a {type(target_list)}.\")\n",
    "            proceed = 0\n",
    "        if not isinstance(feature_list, list):\n",
    "            print(f\"FAILURE: The second object (feature_list) submitted to the function must be a list that has the column names from the dataframe that you want to define as feature variables. You are currently submitting a {type(feature_list)}.\")\n",
    "            proceed = 0\n",
    "        if not isinstance(target_list, list):\n",
    "            print(f\"FAILURE: The third object (target_list) submitted to the function must be a list that has the column names from the dataframe that you want to define as target variables. You are currently submitting a {type(target_list)}.\")\n",
    "            proceed = 0\n",
    "        if proceed == 0:\n",
    "            print(\"Exiting function because of improper input data\")\n",
    "            return\n",
    "        \n",
    "        \n",
    "        # Begin processing data \n",
    "        # Instantiate dataloader dataframe\n",
    "        dataloader_df = pd.DataFrame(columns=[\n",
    "        'feature', 'target', 'data_points', 'avg_p_value', \n",
    "        'lr_slope', 'lr_intercept', 'lr_mse', 'lr_r2', 'lr_avg_test_probability', 'lr_avg_null_probability', 'lr_t_statistic', 'lr_p_value', \n",
    "        'pr_coefficient_1', 'pr_coefficient_2', 'pr_intercept', 'pr_mse', 'pr_r2', 'pr_avg_test_probability', 'pr_avg_null_probability', 'pr_t_statistic', 'pr_p_value', \n",
    "        'att_threshold', 'att_high_target_feature_count', 'att_low_target_feature_count', 'att_high_target_feature_avg', 'att_low_target_feature_avg', 'att_t_statistic', 'att_p_value'\n",
    "        ])\n",
    "        \n",
    "        # Calculate # of values\n",
    "        print(f\"Processing {len(feature_list)} feature values and {len(target_list)} target values for a total of {len(feature_list)*len(target_list)} combinations.\")\n",
    "        \n",
    "        # Initiate Loop\n",
    "        target_count = 0\n",
    "        for target in target_list:\n",
    "            # Report progress\n",
    "            target_count += 1\n",
    "            print(f\"Currently processing target {target_count} of {len(target_list)} total targets.\", end='\\r')\n",
    "            \n",
    "            # Create target_df\n",
    "            target_df = df_to_use[[target]].copy()\n",
    "            \n",
    "            for feature in feature_list:\n",
    "                feature_df = df_to_use[[feature]].copy()\n",
    "                \n",
    "                # Check whether data is valid\n",
    "                valid_data = valid_data_checker(feature_df, target_df)\n",
    "                \n",
    "                if valid_data == 1:\n",
    "                    # Execute models if data is valid\n",
    "                    #print(f\"Processing Data for Feature = {feature_df.columns[0]} and Target = {target_df.columns[0]} \")\n",
    "                    linear_regression_results = execute_linear_regression(feature_df, target_df)\n",
    "                    polynomial_regression_results = execute_polynomial_regression(feature_df, target_df)\n",
    "                    autobucket_t_test_results = autobucket_t_test(feature_df, target_df)\n",
    "                \n",
    "                    # Get the p value average for the 3 tests\n",
    "                    avg_p_value = (linear_regression_results['M_TEST_P_VALUE'][0] + polynomial_regression_results['M_TEST_P_VALUE'][0] + autobucket_t_test_results['p_Value'][0]) / 3\n",
    "                \n",
    "                    # Create Data Loader \n",
    "                    dataloader = {\n",
    "                        'feature': feature_df.columns[0],\n",
    "                        'target': target_df.columns[0],\n",
    "                        'data_points': len(feature_df),\n",
    "                        'avg_p_value': avg_p_value, \n",
    "                        'lr_slope': linear_regression_results['slope'][0], \n",
    "                        'lr_intercept': linear_regression_results['intercept'][0], \n",
    "                        'lr_mse': linear_regression_results['mse'][0], \n",
    "                        'lr_r2': linear_regression_results['r2'][0], \n",
    "                        'lr_avg_test_probability': linear_regression_results['AVG_TEST_PROBABILITY'][0], \n",
    "                        'lr_avg_null_probability': linear_regression_results['AVG_NULL_PROBABILITY'][0], \n",
    "                        'lr_t_statistic': linear_regression_results['T_STATISTIC'][0], \n",
    "                        'lr_p_value': linear_regression_results['M_TEST_P_VALUE'][0], \n",
    "                \n",
    "                        'pr_coefficient_1': polynomial_regression_results['coefficient1'][0], \n",
    "                        'pr_coefficient_2': polynomial_regression_results['coefficient2'][0], \n",
    "                        'pr_intercept': polynomial_regression_results['intercept'][0], \n",
    "                        'pr_mse': polynomial_regression_results['mse'][0], \n",
    "                        'pr_r2': polynomial_regression_results['r2'][0], \n",
    "                        'pr_avg_test_probability': polynomial_regression_results['AVG_TEST_PROBABILITY'][0], \n",
    "                        'pr_avg_null_probability': polynomial_regression_results['AVG_NULL_PROBABILITY'][0], \n",
    "                        'pr_t_statistic': polynomial_regression_results['T_STATISTIC'][0], \n",
    "                        'pr_p_value': polynomial_regression_results['M_TEST_P_VALUE'][0], \n",
    "                \n",
    "                        'att_threshold': autobucket_t_test_results['Threshold'][0], \n",
    "                        'att_high_target_feature_count': autobucket_t_test_results['High_Target_Feature_Count'][0], \n",
    "                        'att_low_target_feature_count': autobucket_t_test_results['Low_Target_Feature_Count'][0], \n",
    "                        'att_high_target_feature_avg': autobucket_t_test_results['High_Target_Feature_Avg'][0], \n",
    "                        'att_low_target_feature_avg': autobucket_t_test_results['Low_Target_Feature_Avg'][0], \n",
    "                        'att_t_statistic': autobucket_t_test_results['T_Statistic'][0], \n",
    "                        'att_p_value': autobucket_t_test_results['p_Value'][0]\n",
    "                    }\n",
    "                \n",
    "                    # Append the new data to the DataFrame\n",
    "                    dataloader_df = pd.concat([dataloader_df, pd.DataFrame([dataloader])], ignore_index=True, sort=False)\n",
    "                \n",
    "                    #print(f\"Dataloader df: {dataloader}; datatype = {type(dataloader)}\")\n",
    "                else:\n",
    "                    print(f\"Skipping Data for Feature = {feature_df.columns[0]} and Target = {target_df.columns[0]} b/c of invalid data\")\n",
    "                \n",
    "                # Export Data \n",
    "                # Export the DataFrame to CSV\n",
    "        dataloader_df.to_csv(export_file_path_string, index=False)\n",
    "        print(f\"Processing complete and results exported to: {export_file_path_string}\")\n",
    "    \n",
    "    except ExceptionType as e:\n",
    "            # Code to handle the exception\n",
    "            print(f\"An exception of type {type(e).__name__} occurred when running execute_feature_target_pair_analysis() function: {str(e)}\")\n",
    "\n",
    "\n",
    "# Execute the function \n",
    "execute_feature_target_pair_analysis(df_to_use, feature_list, target_list, export_file_path_string) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fc1bcd-f5a9-46ec-bac3-3ec027787ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe166ed2-f7b8-4689-a3ff-a92c15573ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3404c0df-53bf-4a02-896b-b2dbddb6cd94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
